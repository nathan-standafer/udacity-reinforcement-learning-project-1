{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#only run this once.  If ran again, the workspace must be restarted.\n",
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up initial environment \n",
    "The cell below instantiates the environment and sets some initial variables:\n",
    "- brain_name\n",
    "- action_size: the number of actions that can be performed in the environment\n",
    "- state_size: the number of values retured from the envionment to represent the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain Name: BananaBrain\n",
      "Number of agents: 1\n",
      "Number of actions (action_size): 4\n",
      "State size (state_size): 37\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "from workspace_utils import active_session\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "\n",
    "#collect infomration about the envronment\n",
    "# reset the environment\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print('Brain Name:', brain_name)\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions (action_size):', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "#print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('State size (state_size):', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "Define the NN model with:\n",
    " - action_size: the number of actions that can be performed in the environment\n",
    " - state_size: the number of values retured from the envionment to represent the current state\n",
    " \n",
    "A helper function (soft_update_target) is also included to update a target model with weights from the local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#define the NN Model\n",
    "class BananaModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_count):\n",
    "        super(BananaModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_count = action_count\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        self.out = torch.nn.Linear(128, action_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "def soft_update_target(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes and functions used when training the model\n",
    " - BananaStepInfo class:  Store information (state, next state, action, reward) about a single step performed in the environment\n",
    " - play_game function: Play a single game, adding a new BananaStepInfo to the history with each step.  Returns the final score to help measure the performance of the model.\n",
    " - get_training_batch: Get a batch of random BananaStepInfo instances for training.  This could be modified for prioritized experienced replay, but for now, its just a random sampling from the history buffer.  Returns a a tuple of lists that are convenient for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class BananaStepInfo():\n",
    "        def __init__(self, action, reward, state, next_state, done):\n",
    "            self.action = action\n",
    "            self.reward = reward\n",
    "            self.state = state\n",
    "            self.next_state = next_state\n",
    "            self.done = done\n",
    "            \n",
    "        def to_string(self, show_states=False):\n",
    "            if show_states:\n",
    "                val = \"BananaStepInfo[action: {}, reward: {}, done: {}, state: {}, next_state: {}]\".format(self.action, self.reward, self.done, self.state, self.next_state)\n",
    "            else:\n",
    "                val = \"BananaStepInfo[action: {}, reward: {}, done: {}]\".format(self.action, self.reward, self.done)\n",
    "            return val\n",
    "\n",
    "def play_game(env, banana_model, epsilon, brain_name, history):\n",
    "    score = 0\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    \n",
    "    while True:\n",
    "        # choose an action using epsilon as the probability of the action being random or greedy from the model\n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand < epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            state_tensor = torch.from_numpy(state).float().cuda()\n",
    "            state_tensor = state_tensor.view((1,)+state.shape) #reshape for batch size of 1\n",
    "            output = banana_model(state_tensor)\n",
    "            action_array = output.detach().cpu().numpy()[0]\n",
    "            action = np.argmax(action_array)\n",
    "            \n",
    "        #take a step, adding informatio about the env to ths history replay buffer.\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        score += reward                                # track total score\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        \n",
    "        bananaStepInfo = BananaStepInfo(action, reward, state, next_state, done)\n",
    "        history.append(bananaStepInfo)\n",
    "        state = next_state\n",
    "\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "    return score\n",
    "            \n",
    "def get_training_batch(batch_size, history):\n",
    "    #generate a random array of indices to select from the history replay buffer\n",
    "    rand_arr = np.arange(len(history))\n",
    "    np.random.shuffle(rand_arr)\n",
    "   \n",
    "    index_counter = 0\n",
    "    batch_index_counter = 0\n",
    "    \n",
    "    state_batch        = np.zeros((batch_size, state_size))\n",
    "    next_state_batch   = np.zeros((batch_size, state_size))\n",
    "    reward_batch       = np.zeros(batch_size)\n",
    "    actions_batch      = np.zeros(batch_size)\n",
    "    done_batch         = np.zeros(batch_size)\n",
    "    \n",
    "    #choose random BananaStepInfo and add to return arrays\n",
    "    for batch_index in range(batch_size):\n",
    "        frame_number = rand_arr[batch_index]\n",
    "        bananaStepInfo = history[frame_number]\n",
    "        \n",
    "        state_batch[batch_index]      = bananaStepInfo.state\n",
    "        next_state_batch[batch_index] = bananaStepInfo.next_state\n",
    "        reward_batch[batch_index]     = bananaStepInfo.reward\n",
    "        actions_batch[batch_index]    = bananaStepInfo.action\n",
    "        done_batch[batch_index]       = bananaStepInfo.done\n",
    "\n",
    "    return state_batch, next_state_batch, actions_batch, reward_batch, done_batch\n",
    "\n",
    "#test out the functions\n",
    "# step_history = deque(maxlen=2000)\n",
    "       \n",
    "# model = BananaModel(state_size, action_size).cuda()\n",
    "# score = play_game(env, model, .5, brain_name, step_history)\n",
    "# print(\"score: \".format(score))\n",
    "\n",
    "# state_batch, next_state_batch, actions_batch, reward_batch, done_batch = get_training_batch(64, step_history)\n",
    "\n",
    "# for i, banana_step in enumerate(step_history):\n",
    "#     print(\"{}: {}\".format(i, banana_step.to_string(show_states=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the models\n",
    "Create a local and two target models.  In this case, I am creating two target models to implement a Double DQN training strategy.  Also instiate the step history so the training loop can be repeated without resetting the models and replay history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create models\n",
    "banana_model_local    = BananaModel(state_size, action_size).cuda()\n",
    "banana_model_target_1 = BananaModel(state_size, action_size).cuda()\n",
    "banana_model_target_2 = BananaModel(state_size, action_size).cuda()\n",
    "\n",
    "#instantiate step history with a maximum of playthrough 4000 steps\n",
    "step_history = deque(maxlen=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Execute the cell below to train the model.  The average game score is printed out after each epoch is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,  score: 4.9, loss: 0.037, epsilon: 0.200\n",
      "epoch: 1,  score: 6.0, loss: 0.038, epsilon: 0.198\n",
      "epoch: 2,  score: 5.8, loss: 0.036, epsilon: 0.196\n",
      "epoch: 3,  score: 6.1, loss: 0.046, epsilon: 0.194\n",
      "epoch: 4,  score: 9.3, loss: 0.051, epsilon: 0.192\n",
      "epoch: 5,  score: 11.0, loss: 0.044, epsilon: 0.190\n",
      "epoch: 6,  score: 6.0, loss: 0.042, epsilon: 0.188\n",
      "epoch: 7,  score: 7.1, loss: 0.041, epsilon: 0.186\n",
      "epoch: 8,  score: 10.2, loss: 0.046, epsilon: 0.184\n",
      "epoch: 9,  score: 8.8, loss: 0.043, epsilon: 0.182\n",
      "epoch: 10,  score: 9.7, loss: 0.045, epsilon: 0.180\n",
      "epoch: 11,  score: 9.3, loss: 0.050, epsilon: 0.178\n",
      "epoch: 12,  score: 9.1, loss: 0.042, epsilon: 0.176\n",
      "epoch: 13,  score: 6.4, loss: 0.046, epsilon: 0.174\n",
      "epoch: 14,  score: 4.6, loss: 0.035, epsilon: 0.172\n",
      "epoch: 15,  score: 5.8, loss: 0.037, epsilon: 0.170\n",
      "epoch: 16,  score: 6.0, loss: 0.031, epsilon: 0.168\n",
      "epoch: 17,  score: 11.1, loss: 0.047, epsilon: 0.166\n",
      "epoch: 18,  score: 9.8, loss: 0.044, epsilon: 0.164\n",
      "epoch: 19,  score: 11.8, loss: 0.040, epsilon: 0.162\n",
      "epoch: 20,  score: 8.1, loss: 0.058, epsilon: 0.160\n",
      "epoch: 21,  score: 11.3, loss: 0.042, epsilon: 0.158\n",
      "epoch: 22,  score: 9.4, loss: 0.056, epsilon: 0.156\n",
      "epoch: 23,  score: 9.4, loss: 0.076, epsilon: 0.154\n",
      "epoch: 24,  score: 9.4, loss: 0.062, epsilon: 0.152\n",
      "epoch: 25,  score: 8.8, loss: 0.054, epsilon: 0.150\n",
      "epoch: 26,  score: 8.6, loss: 0.044, epsilon: 0.148\n",
      "epoch: 27,  score: 7.4, loss: 0.040, epsilon: 0.146\n",
      "epoch: 28,  score: 7.6, loss: 0.040, epsilon: 0.144\n",
      "epoch: 29,  score: 11.9, loss: 0.065, epsilon: 0.142\n",
      "epoch: 30,  score: 9.6, loss: 0.060, epsilon: 0.140\n",
      "epoch: 31,  score: 10.3, loss: 0.071, epsilon: 0.138\n",
      "epoch: 32,  score: 11.1, loss: 0.066, epsilon: 0.136\n",
      "epoch: 33,  score: 10.6, loss: 0.059, epsilon: 0.134\n",
      "epoch: 34,  score: 8.5, loss: 0.060, epsilon: 0.132\n",
      "epoch: 35,  score: 9.0, loss: 0.049, epsilon: 0.130\n",
      "epoch: 36,  score: 7.3, loss: 0.041, epsilon: 0.128\n",
      "epoch: 37,  score: 10.5, loss: 0.041, epsilon: 0.126\n",
      "epoch: 38,  score: 9.1, loss: 0.053, epsilon: 0.124\n",
      "epoch: 39,  score: 10.1, loss: 0.049, epsilon: 0.122\n",
      "epoch: 40,  score: 7.9, loss: 0.042, epsilon: 0.120\n",
      "epoch: 41,  score: 11.8, loss: 0.048, epsilon: 0.118\n",
      "epoch: 42,  score: 13.6, loss: 0.074, epsilon: 0.116\n",
      "epoch: 43,  score: 12.8, loss: 0.073, epsilon: 0.114\n",
      "epoch: 44,  score: 11.7, loss: 0.079, epsilon: 0.112\n",
      "epoch: 45,  score: 12.0, loss: 0.060, epsilon: 0.110\n",
      "epoch: 46,  score: 12.7, loss: 0.069, epsilon: 0.108\n",
      "epoch: 47,  score: 13.2, loss: 0.078, epsilon: 0.106\n",
      "epoch: 48,  score: 11.6, loss: 0.084, epsilon: 0.104\n",
      "epoch: 49,  score: 12.5, loss: 0.055, epsilon: 0.102\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 50,  score: 15.4, loss: 0.068, epsilon: 0.100\n",
      "epoch: 51,  score: 8.4, loss: 0.071, epsilon: 0.098\n",
      "epoch: 52,  score: 12.0, loss: 0.042, epsilon: 0.096\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 53,  score: 15.0, loss: 0.062, epsilon: 0.094\n",
      "epoch: 54,  score: 12.8, loss: 0.077, epsilon: 0.092\n",
      "epoch: 55,  score: 10.4, loss: 0.064, epsilon: 0.090\n",
      "epoch: 56,  score: 8.5, loss: 0.036, epsilon: 0.088\n",
      "epoch: 57,  score: 8.0, loss: 0.053, epsilon: 0.086\n",
      "epoch: 58,  score: 11.8, loss: 0.046, epsilon: 0.084\n",
      "epoch: 59,  score: 12.7, loss: 0.046, epsilon: 0.082\n",
      "epoch: 60,  score: 9.1, loss: 0.059, epsilon: 0.080\n",
      "epoch: 61,  score: 11.6, loss: 0.057, epsilon: 0.078\n",
      "epoch: 62,  score: 12.1, loss: 0.057, epsilon: 0.076\n",
      "epoch: 63,  score: 10.4, loss: 0.062, epsilon: 0.074\n",
      "epoch: 64,  score: 10.5, loss: 0.067, epsilon: 0.072\n",
      "epoch: 65,  score: 12.2, loss: 0.048, epsilon: 0.070\n",
      "epoch: 66,  score: 8.7, loss: 0.058, epsilon: 0.068\n",
      "epoch: 67,  score: 5.8, loss: 0.032, epsilon: 0.066\n",
      "epoch: 68,  score: 11.5, loss: 0.041, epsilon: 0.064\n",
      "epoch: 69,  score: 9.1, loss: 0.061, epsilon: 0.062\n",
      "epoch: 70,  score: 7.6, loss: 0.061, epsilon: 0.060\n",
      "epoch: 71,  score: 5.5, loss: 0.054, epsilon: 0.058\n",
      "epoch: 72,  score: 8.3, loss: 0.026, epsilon: 0.056\n",
      "epoch: 73,  score: 10.8, loss: 0.030, epsilon: 0.054\n",
      "epoch: 74,  score: 10.7, loss: 0.045, epsilon: 0.052\n",
      "epoch: 75,  score: 8.7, loss: 0.051, epsilon: 0.050\n",
      "epoch: 76,  score: 10.2, loss: 0.052, epsilon: 0.050\n",
      "epoch: 77,  score: 10.9, loss: 0.076, epsilon: 0.050\n",
      "epoch: 78,  score: 12.4, loss: 0.054, epsilon: 0.050\n",
      "epoch: 79,  score: 13.9, loss: 0.070, epsilon: 0.050\n",
      "epoch: 80,  score: 11.9, loss: 0.070, epsilon: 0.050\n",
      "epoch: 81,  score: 11.3, loss: 0.062, epsilon: 0.050\n",
      "epoch: 82,  score: 11.2, loss: 0.059, epsilon: 0.050\n",
      "epoch: 83,  score: 12.9, loss: 0.047, epsilon: 0.050\n",
      "epoch: 84,  score: 10.7, loss: 0.069, epsilon: 0.050\n",
      "epoch: 85,  score: 14.0, loss: 0.049, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 86,  score: 14.1, loss: 0.062, epsilon: 0.050\n",
      "epoch: 87,  score: 13.8, loss: 0.055, epsilon: 0.050\n",
      "epoch: 88,  score: 11.9, loss: 0.082, epsilon: 0.050\n",
      "epoch: 89,  score: 8.0, loss: 0.056, epsilon: 0.050\n",
      "epoch: 90,  score: 13.7, loss: 0.059, epsilon: 0.050\n",
      "epoch: 91,  score: 13.8, loss: 0.098, epsilon: 0.050\n",
      "epoch: 92,  score: 12.9, loss: 0.059, epsilon: 0.050\n",
      "epoch: 93,  score: 12.9, loss: 0.067, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 94,  score: 14.2, loss: 0.072, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 95,  score: 14.7, loss: 0.076, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 96,  score: 14.6, loss: 0.067, epsilon: 0.050\n",
      "epoch: 97,  score: 13.7, loss: 0.065, epsilon: 0.050\n",
      "epoch: 98,  score: 13.6, loss: 0.067, epsilon: 0.050\n",
      "epoch: 99,  score: 12.1, loss: 0.058, epsilon: 0.050\n",
      "epoch: 100,  score: 13.9, loss: 0.064, epsilon: 0.050\n",
      "epoch: 101,  score: 12.0, loss: 0.072, epsilon: 0.050\n",
      "epoch: 102,  score: 10.4, loss: 0.053, epsilon: 0.050\n",
      "epoch: 103,  score: 11.9, loss: 0.059, epsilon: 0.050\n",
      "epoch: 104,  score: 10.1, loss: 0.048, epsilon: 0.050\n",
      "epoch: 105,  score: 11.9, loss: 0.056, epsilon: 0.050\n",
      "epoch: 106,  score: 13.6, loss: 0.057, epsilon: 0.050\n",
      "epoch: 107,  score: 13.2, loss: 0.076, epsilon: 0.050\n",
      "epoch: 108,  score: 14.0, loss: 0.061, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 109,  score: 14.1, loss: 0.064, epsilon: 0.050\n",
      "epoch: 110,  score: 12.1, loss: 0.066, epsilon: 0.050\n",
      "epoch: 111,  score: 12.3, loss: 0.062, epsilon: 0.050\n",
      "epoch: 112,  score: 10.7, loss: 0.061, epsilon: 0.050\n",
      "epoch: 113,  score: 11.9, loss: 0.061, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 114,  score: 15.4, loss: 0.054, epsilon: 0.050\n",
      "epoch: 115,  score: 12.7, loss: 0.064, epsilon: 0.050\n",
      "epoch: 116,  score: 10.3, loss: 0.081, epsilon: 0.050\n",
      "epoch: 117,  score: 10.1, loss: 0.059, epsilon: 0.050\n",
      "epoch: 118,  score: 12.1, loss: 0.061, epsilon: 0.050\n",
      "epoch: 119,  score: 12.9, loss: 0.051, epsilon: 0.050\n",
      "epoch: 120,  score: 14.0, loss: 0.079, epsilon: 0.050\n",
      "epoch: 121,  score: 12.6, loss: 0.071, epsilon: 0.050\n",
      "epoch: 122,  score: 11.1, loss: 0.053, epsilon: 0.050\n",
      "epoch: 123,  score: 11.2, loss: 0.069, epsilon: 0.050\n",
      "epoch: 124,  score: 10.8, loss: 0.056, epsilon: 0.050\n",
      "epoch: 125,  score: 13.2, loss: 0.060, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 126,  score: 15.6, loss: 0.069, epsilon: 0.050\n",
      "epoch: 127,  score: 10.7, loss: 0.066, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 128,  score: 14.3, loss: 0.057, epsilon: 0.050\n",
      "epoch: 129,  score: 13.7, loss: 0.078, epsilon: 0.050\n",
      "epoch: 130,  score: 14.0, loss: 0.069, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 131,  score: 14.1, loss: 0.058, epsilon: 0.050\n",
      "epoch: 132,  score: 12.0, loss: 0.078, epsilon: 0.050\n",
      "epoch: 133,  score: 13.1, loss: 0.067, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 134,  score: 14.2, loss: 0.069, epsilon: 0.050\n",
      "epoch: 135,  score: 9.7, loss: 0.063, epsilon: 0.050\n",
      "epoch: 136,  score: 11.0, loss: 0.055, epsilon: 0.050\n",
      "epoch: 137,  score: 12.9, loss: 0.044, epsilon: 0.050\n",
      "epoch: 138,  score: 11.9, loss: 0.063, epsilon: 0.050\n",
      "epoch: 139,  score: 11.7, loss: 0.075, epsilon: 0.050\n",
      "epoch: 140,  score: 9.2, loss: 0.053, epsilon: 0.050\n",
      "epoch: 141,  score: 10.3, loss: 0.051, epsilon: 0.050\n",
      "epoch: 142,  score: 9.8, loss: 0.061, epsilon: 0.050\n",
      "epoch: 143,  score: 10.9, loss: 0.051, epsilon: 0.050\n",
      "epoch: 144,  score: 12.2, loss: 0.071, epsilon: 0.050\n",
      "epoch: 145,  score: 12.4, loss: 0.082, epsilon: 0.050\n",
      "epoch: 146,  score: 12.8, loss: 0.083, epsilon: 0.050\n",
      "epoch: 147,  score: 10.5, loss: 0.071, epsilon: 0.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached.  Training stopped.\n",
      "epoch: 148,  score: 14.1, loss: 0.057, epsilon: 0.050\n",
      "epoch: 149,  score: 10.1, loss: 0.072, epsilon: 0.050\n",
      "epoch: 150,  score: 13.2, loss: 0.071, epsilon: 0.050\n",
      "epoch: 151,  score: 11.7, loss: 0.065, epsilon: 0.050\n",
      "epoch: 152,  score: 10.3, loss: 0.072, epsilon: 0.050\n",
      "epoch: 153,  score: 13.7, loss: 0.079, epsilon: 0.050\n",
      "epoch: 154,  score: 11.4, loss: 0.079, epsilon: 0.050\n",
      "epoch: 155,  score: 13.7, loss: 0.074, epsilon: 0.050\n",
      "epoch: 156,  score: 12.9, loss: 0.085, epsilon: 0.050\n",
      "epoch: 157,  score: 14.0, loss: 0.076, epsilon: 0.050\n",
      "epoch: 158,  score: 13.6, loss: 0.058, epsilon: 0.050\n",
      "epoch: 159,  score: 13.9, loss: 0.093, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 160,  score: 15.9, loss: 0.094, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 161,  score: 14.2, loss: 0.100, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 162,  score: 14.4, loss: 0.082, epsilon: 0.050\n",
      "epoch: 163,  score: 13.5, loss: 0.084, epsilon: 0.050\n",
      "epoch: 164,  score: 13.3, loss: 0.084, epsilon: 0.050\n",
      "epoch: 165,  score: 12.2, loss: 0.064, epsilon: 0.050\n",
      "epoch: 166,  score: 13.5, loss: 0.059, epsilon: 0.050\n",
      "epoch: 167,  score: 12.9, loss: 0.073, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 168,  score: 14.1, loss: 0.083, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 169,  score: 14.3, loss: 0.077, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 170,  score: 15.8, loss: 0.098, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 171,  score: 14.9, loss: 0.087, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 172,  score: 15.8, loss: 0.077, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 173,  score: 14.4, loss: 0.077, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 174,  score: 14.8, loss: 0.072, epsilon: 0.050\n",
      "epoch: 175,  score: 14.0, loss: 0.093, epsilon: 0.050\n",
      "epoch: 176,  score: 13.4, loss: 0.069, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 177,  score: 14.1, loss: 0.063, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 178,  score: 14.1, loss: 0.081, epsilon: 0.050\n",
      "epoch: 179,  score: 11.4, loss: 0.067, epsilon: 0.050\n",
      "epoch: 180,  score: 12.3, loss: 0.061, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 181,  score: 15.7, loss: 0.079, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 182,  score: 14.3, loss: 0.083, epsilon: 0.050\n",
      "epoch: 183,  score: 13.5, loss: 0.078, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 184,  score: 14.4, loss: 0.061, epsilon: 0.050\n",
      "epoch: 185,  score: 13.1, loss: 0.073, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 186,  score: 14.1, loss: 0.089, epsilon: 0.050\n",
      "epoch: 187,  score: 13.0, loss: 0.083, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 188,  score: 14.4, loss: 0.078, epsilon: 0.050\n",
      "epoch: 189,  score: 13.0, loss: 0.073, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 190,  score: 14.8, loss: 0.064, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 191,  score: 14.3, loss: 0.074, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 192,  score: 15.3, loss: 0.069, epsilon: 0.050\n",
      "epoch: 193,  score: 13.2, loss: 0.073, epsilon: 0.050\n",
      "epoch: 194,  score: 13.4, loss: 0.075, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 195,  score: 14.5, loss: 0.085, epsilon: 0.050\n",
      "epoch: 196,  score: 13.9, loss: 0.086, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 197,  score: 14.7, loss: 0.079, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 198,  score: 14.2, loss: 0.078, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 199,  score: 15.1, loss: 0.072, epsilon: 0.050\n",
      "epoch: 200,  score: 13.9, loss: 0.069, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 201,  score: 15.0, loss: 0.074, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 202,  score: 14.4, loss: 0.097, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 203,  score: 14.3, loss: 0.092, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 204,  score: 14.4, loss: 0.090, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 205,  score: 15.4, loss: 0.073, epsilon: 0.050\n",
      "epoch: 206,  score: 10.8, loss: 0.089, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 207,  score: 14.9, loss: 0.076, epsilon: 0.050\n",
      "epoch: 208,  score: 12.2, loss: 0.091, epsilon: 0.050\n",
      "epoch: 209,  score: 12.0, loss: 0.080, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 210,  score: 15.3, loss: 0.068, epsilon: 0.050\n",
      "epoch: 211,  score: 11.4, loss: 0.073, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 212,  score: 14.7, loss: 0.066, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 213,  score: 14.7, loss: 0.071, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 214,  score: 14.1, loss: 0.073, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 215,  score: 14.2, loss: 0.057, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 216,  score: 14.4, loss: 0.078, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 217,  score: 14.8, loss: 0.079, epsilon: 0.050\n",
      "epoch: 218,  score: 10.7, loss: 0.073, epsilon: 0.050\n",
      "epoch: 219,  score: 13.9, loss: 0.064, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 220,  score: 15.0, loss: 0.083, epsilon: 0.050\n",
      "epoch: 221,  score: 12.1, loss: 0.087, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 222,  score: 14.6, loss: 0.069, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 223,  score: 14.4, loss: 0.084, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 224,  score: 14.1, loss: 0.098, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 225,  score: 14.3, loss: 0.067, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 226,  score: 15.6, loss: 0.090, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 227,  score: 14.6, loss: 0.088, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 228,  score: 14.6, loss: 0.091, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 229,  score: 14.3, loss: 0.085, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 230,  score: 14.2, loss: 0.079, epsilon: 0.050\n",
      "epoch: 231,  score: 12.9, loss: 0.084, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 232,  score: 14.3, loss: 0.082, epsilon: 0.050\n",
      "epoch: 233,  score: 11.8, loss: 0.074, epsilon: 0.050\n",
      "epoch: 234,  score: 13.9, loss: 0.062, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 235,  score: 14.2, loss: 0.068, epsilon: 0.050\n",
      "epoch: 236,  score: 12.5, loss: 0.058, epsilon: 0.050\n",
      "epoch: 237,  score: 12.1, loss: 0.067, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 238,  score: 15.0, loss: 0.070, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 239,  score: 14.7, loss: 0.093, epsilon: 0.050\n",
      "epoch: 240,  score: 14.0, loss: 0.076, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 241,  score: 14.6, loss: 0.095, epsilon: 0.050\n",
      "epoch: 242,  score: 13.8, loss: 0.069, epsilon: 0.050\n",
      "epoch: 243,  score: 13.6, loss: 0.075, epsilon: 0.050\n",
      "epoch: 244,  score: 12.8, loss: 0.077, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 245,  score: 14.3, loss: 0.077, epsilon: 0.050\n",
      "epoch: 246,  score: 13.0, loss: 0.079, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 247,  score: 15.2, loss: 0.064, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 248,  score: 15.5, loss: 0.081, epsilon: 0.050\n",
      "epoch: 249,  score: 13.6, loss: 0.080, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 250,  score: 15.2, loss: 0.079, epsilon: 0.050\n",
      "epoch: 251,  score: 13.5, loss: 0.105, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 252,  score: 14.6, loss: 0.077, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 253,  score: 15.8, loss: 0.093, epsilon: 0.050\n",
      "epoch: 254,  score: 12.4, loss: 0.091, epsilon: 0.050\n",
      "epoch: 255,  score: 11.5, loss: 0.080, epsilon: 0.050\n",
      "epoch: 256,  score: 10.7, loss: 0.059, epsilon: 0.050\n",
      "epoch: 257,  score: 12.6, loss: 0.056, epsilon: 0.050\n",
      "epoch: 258,  score: 11.8, loss: 0.068, epsilon: 0.050\n",
      "epoch: 259,  score: 12.3, loss: 0.060, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 260,  score: 15.0, loss: 0.063, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 261,  score: 14.5, loss: 0.080, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 262,  score: 15.3, loss: 0.086, epsilon: 0.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached.  Training stopped.\n",
      "epoch: 263,  score: 15.5, loss: 0.102, epsilon: 0.050\n",
      "epoch: 264,  score: 13.1, loss: 0.111, epsilon: 0.050\n",
      "epoch: 265,  score: 13.1, loss: 0.092, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 266,  score: 14.4, loss: 0.090, epsilon: 0.050\n",
      "epoch: 267,  score: 9.7, loss: 0.067, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 268,  score: 15.0, loss: 0.071, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 269,  score: 15.1, loss: 0.095, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 270,  score: 15.5, loss: 0.096, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 271,  score: 15.1, loss: 0.104, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 272,  score: 15.3, loss: 0.095, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 273,  score: 14.7, loss: 0.070, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 274,  score: 14.3, loss: 0.084, epsilon: 0.050\n",
      "epoch: 275,  score: 13.6, loss: 0.093, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 276,  score: 14.5, loss: 0.092, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 277,  score: 14.1, loss: 0.083, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 278,  score: 15.2, loss: 0.098, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 279,  score: 14.8, loss: 0.076, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 280,  score: 14.4, loss: 0.075, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 281,  score: 14.4, loss: 0.068, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 282,  score: 14.5, loss: 0.083, epsilon: 0.050\n",
      "epoch: 283,  score: 13.8, loss: 0.087, epsilon: 0.050\n",
      "epoch: 284,  score: 13.5, loss: 0.070, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 285,  score: 14.1, loss: 0.095, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 286,  score: 14.7, loss: 0.090, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 287,  score: 15.0, loss: 0.082, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 288,  score: 14.1, loss: 0.074, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 289,  score: 15.4, loss: 0.092, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 290,  score: 14.8, loss: 0.102, epsilon: 0.050\n",
      "epoch: 291,  score: 13.1, loss: 0.116, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 292,  score: 14.9, loss: 0.089, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 293,  score: 14.3, loss: 0.078, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 294,  score: 14.1, loss: 0.084, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 295,  score: 14.1, loss: 0.083, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 296,  score: 15.0, loss: 0.117, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 297,  score: 15.4, loss: 0.091, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 298,  score: 15.2, loss: 0.095, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 299,  score: 14.8, loss: 0.082, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 300,  score: 14.1, loss: 0.078, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 301,  score: 14.8, loss: 0.078, epsilon: 0.050\n",
      "epoch: 302,  score: 11.7, loss: 0.068, epsilon: 0.050\n",
      "epoch: 303,  score: 13.5, loss: 0.076, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 304,  score: 14.6, loss: 0.080, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 305,  score: 15.3, loss: 0.093, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 306,  score: 14.8, loss: 0.090, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 307,  score: 14.5, loss: 0.095, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 308,  score: 14.9, loss: 0.097, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 309,  score: 14.8, loss: 0.111, epsilon: 0.050\n",
      "epoch: 310,  score: 11.0, loss: 0.088, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 311,  score: 14.5, loss: 0.066, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 312,  score: 15.2, loss: 0.107, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 313,  score: 14.7, loss: 0.080, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 314,  score: 14.2, loss: 0.074, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 315,  score: 14.8, loss: 0.091, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 316,  score: 14.2, loss: 0.061, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 317,  score: 14.8, loss: 0.075, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 318,  score: 15.7, loss: 0.079, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 319,  score: 15.5, loss: 0.115, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 320,  score: 15.5, loss: 0.104, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 321,  score: 14.8, loss: 0.105, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 322,  score: 15.1, loss: 0.100, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 323,  score: 14.5, loss: 0.073, epsilon: 0.050\n",
      "epoch: 324,  score: 13.6, loss: 0.098, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 325,  score: 14.5, loss: 0.068, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 326,  score: 14.2, loss: 0.071, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 327,  score: 15.1, loss: 0.088, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 328,  score: 15.2, loss: 0.100, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 329,  score: 14.3, loss: 0.085, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 330,  score: 15.3, loss: 0.065, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 331,  score: 15.0, loss: 0.082, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 332,  score: 14.9, loss: 0.089, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 333,  score: 15.3, loss: 0.099, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 334,  score: 14.1, loss: 0.067, epsilon: 0.050\n",
      "epoch: 335,  score: 13.9, loss: 0.086, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 336,  score: 14.4, loss: 0.092, epsilon: 0.050\n",
      "epoch: 337,  score: 12.5, loss: 0.067, epsilon: 0.050\n",
      "epoch: 338,  score: 13.3, loss: 0.080, epsilon: 0.050\n",
      "epoch: 339,  score: 9.6, loss: 0.096, epsilon: 0.050\n",
      "epoch: 340,  score: 10.3, loss: 0.091, epsilon: 0.050\n",
      "epoch: 341,  score: 11.0, loss: 0.062, epsilon: 0.050\n",
      "epoch: 342,  score: 6.1, loss: 0.052, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 343,  score: 15.1, loss: 0.067, epsilon: 0.050\n",
      "epoch: 344,  score: 13.6, loss: 0.085, epsilon: 0.050\n",
      "epoch: 345,  score: 12.8, loss: 0.079, epsilon: 0.050\n",
      "epoch: 346,  score: 13.8, loss: 0.089, epsilon: 0.050\n",
      "epoch: 347,  score: 13.6, loss: 0.088, epsilon: 0.050\n",
      "epoch: 348,  score: 13.6, loss: 0.088, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 349,  score: 15.1, loss: 0.072, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 350,  score: 15.9, loss: 0.078, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 351,  score: 14.1, loss: 0.086, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 352,  score: 15.6, loss: 0.077, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 353,  score: 14.2, loss: 0.080, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 354,  score: 15.1, loss: 0.080, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 355,  score: 15.3, loss: 0.074, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 356,  score: 15.5, loss: 0.073, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 357,  score: 14.4, loss: 0.075, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 358,  score: 14.6, loss: 0.090, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 359,  score: 15.4, loss: 0.110, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 360,  score: 14.4, loss: 0.100, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 361,  score: 15.0, loss: 0.108, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 362,  score: 14.2, loss: 0.110, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 363,  score: 14.4, loss: 0.079, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 364,  score: 15.0, loss: 0.075, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 365,  score: 14.2, loss: 0.078, epsilon: 0.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal reached.  Training stopped.\n",
      "epoch: 366,  score: 14.7, loss: 0.086, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 367,  score: 14.4, loss: 0.097, epsilon: 0.050\n",
      "epoch: 368,  score: 13.5, loss: 0.098, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 369,  score: 14.3, loss: 0.064, epsilon: 0.050\n",
      "epoch: 370,  score: 13.9, loss: 0.080, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 371,  score: 15.5, loss: 0.102, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 372,  score: 14.4, loss: 0.094, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 373,  score: 14.7, loss: 0.072, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 374,  score: 14.1, loss: 0.074, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 375,  score: 14.3, loss: 0.093, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 376,  score: 14.6, loss: 0.088, epsilon: 0.050\n",
      "epoch: 377,  score: 13.3, loss: 0.096, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 378,  score: 14.3, loss: 0.085, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 379,  score: 15.4, loss: 0.105, epsilon: 0.050\n",
      "epoch: 380,  score: 13.8, loss: 0.086, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 381,  score: 15.4, loss: 0.106, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 382,  score: 14.9, loss: 0.087, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 383,  score: 15.5, loss: 0.113, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 384,  score: 15.9, loss: 0.097, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 385,  score: 14.4, loss: 0.104, epsilon: 0.050\n",
      "epoch: 386,  score: 12.6, loss: 0.086, epsilon: 0.050\n",
      "epoch: 387,  score: 13.6, loss: 0.095, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 388,  score: 15.0, loss: 0.081, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 389,  score: 14.3, loss: 0.092, epsilon: 0.050\n",
      "epoch: 390,  score: 13.1, loss: 0.082, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 391,  score: 15.6, loss: 0.092, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 392,  score: 14.2, loss: 0.067, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 393,  score: 14.1, loss: 0.102, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 394,  score: 15.1, loss: 0.112, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 395,  score: 14.5, loss: 0.087, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 396,  score: 14.5, loss: 0.078, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 397,  score: 14.3, loss: 0.101, epsilon: 0.050\n",
      "epoch: 398,  score: 13.7, loss: 0.092, epsilon: 0.050\n",
      "Goal reached.  Training stopped.\n",
      "epoch: 399,  score: 15.7, loss: 0.077, epsilon: 0.050\n"
     ]
    }
   ],
   "source": [
    "from workspace_utils import active_session\n",
    "\n",
    "epsilon_max = 0.2            #value of epsilon when training is started\n",
    "epsilon_min = 0.05           #value of epsilon when training has reached epsilon_decay_epochs\n",
    "epsilon_decay_epochs = 75   #number if epochs requred to reach epsilon_min\n",
    "TAU = .005                   #controls degree to which target models are updated after each game.\n",
    "gamma = .99                  #discounted rewards factor\n",
    "\n",
    "epochs = 800                 #how many epochs to run for (200 seems to be about enough)\n",
    "games_per_epoch = 10         #games played per epoch\n",
    "batch_trainings_per_game = 4 #how many training batches are performed after each game (games last about 300 steps, so 4*64 sounds about right)\n",
    "batch_size = 64              #training batch size\n",
    "\n",
    "learning_rate = .0004        #learning rate.  .0004 seems to work OK.  \n",
    "optimizer = torch.optim.Adam(banana_model_local.parameters(), lr=learning_rate)  \n",
    "\n",
    "with active_session():\n",
    "    for epoch in range(epochs):\n",
    "        #calculate epsilon for this epoch.\n",
    "        epsilon = epsilon_max - (epsilon_max-epsilon_min)*(epoch/epsilon_decay_epochs)\n",
    "        epsilon = max(epsilon_min, epsilon)\n",
    "        total_loss = 0\n",
    "        total_score = 0\n",
    "\n",
    "        for game_count in range(games_per_epoch):\n",
    "            #play a game, adding BananaStepInfo steps to step_history\n",
    "            score = play_game(env, banana_model_local, epsilon, brain_name, step_history)\n",
    "            total_score += score\n",
    "\n",
    "            #I found that letting gradient accumulate over the batch_trainings_per_game trains better.\n",
    "            optimizer.zero_grad() #zero out gradients.  \n",
    "\n",
    "            for batch_training_count in range(batch_trainings_per_game):\n",
    "                #get training batch\n",
    "                state_batch, next_state_batch, actions_batch, reward_batch, done_batch = get_training_batch(batch_size, step_history)\n",
    "\n",
    "                #now, implement Double DQN strategy using one target model to get maximum actions and another target \n",
    "                #model to get the expected rewards based upon those actions.  Using the two models in this manner helps \n",
    "                #to ensure rewards that deviate far from reality do not negatively influence training.\n",
    "\n",
    "                #using target model 1, get the greedy actions (actions with greatest reward) for each step in the batch (target_indices_1).\n",
    "                next_state_tensor = torch.from_numpy(next_state_batch).float().cuda()\n",
    "                target_q_values_next_1 = banana_model_target_1(next_state_tensor)\n",
    "                target_q_values_next_max_1, target_indices_1 = torch.max(target_q_values_next_1, dim=1, keepdim=True)\n",
    "\n",
    "                #using the greedy actions (target_indices_1) from target model 1, get the expected rewards from target model 2 (target_q_values_next)\n",
    "                target_q_values_next_2 = banana_model_target_2(next_state_tensor)\n",
    "                target_q_values_next = target_q_values_next_2.gather(1, target_indices_1)\n",
    "\n",
    "                #get the expected current rewards earned using historical actions performed using the banana_model_local\n",
    "                history_actions_batch_reshaped = np.reshape(actions_batch, (batch_size, 1))\n",
    "                history_actions_batch_tensor = torch.from_numpy(history_actions_batch_reshaped).long().cuda()\n",
    "                state_tensor = torch.from_numpy(state_batch).float().cuda()\n",
    "                local_q_values = banana_model_local(state_tensor)  #feed historical states into local model to get q_values for each action \n",
    "                local_q_values_performed = local_q_values.gather(1, history_actions_batch_tensor) #get q_value for each action that was performed from history\n",
    "\n",
    "                #get the actual rewards earned from the playthrough history\n",
    "                reward_batch_reshaped = np.reshape(reward_batch, (batch_size, 1))\n",
    "                reward_batch_tensor = torch.from_numpy(reward_batch_reshaped).float().cuda()\n",
    "\n",
    "                #calculate loss using the deep Q learning equation.  I like seeing the actual equation this way instead of breaking\n",
    "                #it up across multiple lines.\n",
    "                loss = torch.mean((reward_batch_tensor + (gamma * target_q_values_next) - local_q_values_performed)**2)\n",
    "                total_loss += loss.item() #track total loss\n",
    "\n",
    "                loss.backward()   #backpropigation is only performed on the local model\n",
    "                optimizer.step()\n",
    "\n",
    "            #now, randomly choose a taret model to update. Randomly choosing a model makes sure one model is not influenced \n",
    "            #too much to negate the traing stability that Doubel DQN wants to achieve.\n",
    "            rand = random.uniform(0, 1)\n",
    "            if rand < .5:\n",
    "                soft_update_target(banana_model_local, banana_model_target_1, TAU)\n",
    "            else:\n",
    "                soft_update_target(banana_model_local, banana_model_target_2, TAU)\n",
    "\n",
    "        print(\"epoch: {},  score: {}, loss: {:.3f}, epsilon: {:.3f}\".format(epoch, total_score/games_per_epoch, total_loss/games_per_epoch, epsilon))\n",
    "        \n",
    "        if total_score/games_per_epoch > 15.0:\n",
    "            print(\"Goal reached.  Training stopped.\")\n",
    "            break  #if 10 games are played with an average over 14, assume it is solved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you like the model, save it here\n",
    "torch.save(banana_model_local.state_dict(), \"banana_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BananaModel(\n",
      "  (fc1): Linear(in_features=37, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (out): Linear(in_features=128, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#load the weights into a new model to play a game and get the score\n",
    "banana_model_load  = BananaModel(state_size, action_size).cuda()\n",
    "banana_model_load.load_state_dict(torch.load(\"banana_model.pt\"));\n",
    "\n",
    "print(banana_model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 19.0 in 300 steps\n"
     ]
    }
   ],
   "source": [
    "#play a game to see what score is achieved\n",
    "\n",
    "play_history = []\n",
    "score = play_game(env, banana_model_load, .01, brain_name, play_history)\n",
    "\n",
    "print(\"Score {} in {} steps\".format(score, len(play_history)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
