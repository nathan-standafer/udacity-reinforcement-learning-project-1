{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "#only run this once.  If ran again, the workspace must be restarted.\n",
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up initial environment \n",
    "The cell below instantiates the environment and sets some initial variables:\n",
    "- brain_name\n",
    "- action_size: the number of actions that can be performed in the environment\n",
    "- state_size: the number of values retured from the envionment to represent the current state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'unityagents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-352176e3c7ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0munityagents\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnityEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/data/Banana_Linux_NoVis/Banana.x86_64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'unityagents'"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "\n",
    "#collect infomration about the envronment\n",
    "# reset the environment\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print('Brain Name:', brain_name)\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions (action_size):', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "#print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('State size (state_size):', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "Define the NN model with:\n",
    " - action_size: the number of actions that can be performed in the environment\n",
    " - state_size: the number of values retured from the envionment to represent the current state\n",
    " \n",
    "A helper function (soft_update_target) is also included to update a target model with weights from the local model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#define the NN Model\n",
    "class BananaModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_count):\n",
    "        super(BananaModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_count = action_count\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        self.out = torch.nn.Linear(128, action_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "def soft_update_target(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define classes and functions used when training the model\n",
    " - BananaStepInfo class:  Store information (state, next state, action, reward) about a single step performed in the environment\n",
    " - play_game function: Play a single game, adding a new BananaStepInfo to the history with each step.  Returns the final score to help measure the performance of the model.\n",
    " - get_training_batch: Get a batch of random BananaStepInfo instances for training.  This could be modified for prioritized experienced replay, but for now, its just a random sampling from the history buffer.  Returns a a tuple of lists that are convenient for training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class BananaStepInfo():\n",
    "        def __init__(self, action, reward, state, next_state, done):\n",
    "            self.action = action\n",
    "            self.reward = reward\n",
    "            self.state = state\n",
    "            self.next_state = next_state\n",
    "            self.done = done\n",
    "            \n",
    "        def to_string(self, show_states=False):\n",
    "            if show_states:\n",
    "                val = \"BananaStepInfo[action: {}, reward: {}, done: {}, state: {}, next_state: {}]\".format(self.action, self.reward, self.done, self.state, self.next_state)\n",
    "            else:\n",
    "                val = \"BananaStepInfo[action: {}, reward: {}, done: {}]\".format(self.action, self.reward, self.done)\n",
    "            return val\n",
    "\n",
    "def play_game(env, banana_model, epsilon, brain_name, history):\n",
    "    score = 0\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    \n",
    "    while True:\n",
    "        # choose an action using epsilon as the probability of the action being random or greedy from the model\n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand < epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            state_tensor = torch.from_numpy(state).float().cuda()\n",
    "            state_tensor = state_tensor.view((1,)+state.shape) #reshape for batch size of 1\n",
    "            output = banana_model(state_tensor)\n",
    "            action_array = output.detach().cpu().numpy()[0]\n",
    "            action = np.argmax(action_array)\n",
    "            \n",
    "        #take a step, adding informatio about the env to ths history replay buffer.\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        score += reward                                # track total score\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        \n",
    "        bananaStepInfo = BananaStepInfo(action, reward, state, next_state, done)\n",
    "        history.append(bananaStepInfo)\n",
    "        state = next_state\n",
    "\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "    return score\n",
    "            \n",
    "def get_training_batch(batch_size, history):\n",
    "    #generate a random array of indices to select from the history replay buffer\n",
    "    rand_arr = np.arange(len(history))\n",
    "    np.random.shuffle(rand_arr)\n",
    "   \n",
    "    index_counter = 0\n",
    "    batch_index_counter = 0\n",
    "    \n",
    "    state_batch        = np.zeros((batch_size, state_size))\n",
    "    next_state_batch   = np.zeros((batch_size, state_size))\n",
    "    reward_batch       = np.zeros(batch_size)\n",
    "    actions_batch      = np.zeros(batch_size)\n",
    "    done_batch         = np.zeros(batch_size)\n",
    "    \n",
    "    #choose random BananaStepInfo and add to return arrays\n",
    "    for batch_index in range(batch_size):\n",
    "        frame_number = rand_arr[batch_index]\n",
    "        bananaStepInfo = history[frame_number]\n",
    "        \n",
    "        state_batch[batch_index]      = bananaStepInfo.state\n",
    "        next_state_batch[batch_index] = bananaStepInfo.next_state\n",
    "        reward_batch[batch_index]     = bananaStepInfo.reward\n",
    "        actions_batch[batch_index]    = bananaStepInfo.action\n",
    "        done_batch[batch_index]       = bananaStepInfo.done\n",
    "\n",
    "    return state_batch, next_state_batch, actions_batch, reward_batch, done_batch\n",
    "\n",
    "#test out the functions\n",
    "# step_history = deque(maxlen=2000)\n",
    "       \n",
    "# model = BananaModel(state_size, action_size).cuda()\n",
    "# score = play_game(env, model, .5, brain_name, step_history)\n",
    "# print(\"score: \".format(score))\n",
    "\n",
    "# state_batch, next_state_batch, actions_batch, reward_batch, done_batch = get_training_batch(64, step_history)\n",
    "\n",
    "# for i, banana_step in enumerate(step_history):\n",
    "#     print(\"{}: {}\".format(i, banana_step.to_string(show_states=False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the models\n",
    "Create a local and two target models.  In this case, I am creating two target models to implement A Double DQN training strategy.  Also instiate the step history so the training loop can be repeated without resetting the models and replay history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create models\n",
    "banana_model_local    = BananaModel(state_size, action_size).cuda()\n",
    "banana_model_target_1 = BananaModel(state_size, action_size).cuda()\n",
    "banana_model_target_2 = BananaModel(state_size, action_size).cuda()\n",
    "\n",
    "#instantiate step history with a maximum of playthrough 4000 steps\n",
    "step_history = deque(maxlen=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model\n",
    "Execute the cell below to train the model.  The average game score is printed out after each epoch is completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,  score: 10.8, loss: 0.056, epsilon: 0.100\n",
      "epoch: 1,  score: 5.7, loss: 0.047, epsilon: 0.100\n",
      "epoch: 2,  score: 6.9, loss: 0.037, epsilon: 0.099\n",
      "epoch: 3,  score: 9.8, loss: 0.038, epsilon: 0.099\n",
      "epoch: 4,  score: 10.5, loss: 0.053, epsilon: 0.099\n",
      "epoch: 5,  score: 6.7, loss: 0.057, epsilon: 0.098\n",
      "epoch: 6,  score: 8.3, loss: 0.033, epsilon: 0.098\n",
      "epoch: 7,  score: 9.4, loss: 0.043, epsilon: 0.098\n",
      "epoch: 8,  score: 8.8, loss: 0.039, epsilon: 0.097\n",
      "epoch: 9,  score: 9.7, loss: 0.063, epsilon: 0.097\n",
      "epoch: 10,  score: 10.9, loss: 0.053, epsilon: 0.097\n",
      "epoch: 11,  score: 10.4, loss: 0.050, epsilon: 0.096\n",
      "epoch: 12,  score: 9.6, loss: 0.057, epsilon: 0.096\n",
      "epoch: 13,  score: 8.1, loss: 0.052, epsilon: 0.096\n",
      "epoch: 14,  score: 10.9, loss: 0.050, epsilon: 0.095\n",
      "epoch: 15,  score: 5.5, loss: 0.043, epsilon: 0.095\n",
      "epoch: 16,  score: 5.7, loss: 0.033, epsilon: 0.095\n",
      "epoch: 17,  score: 8.8, loss: 0.050, epsilon: 0.094\n",
      "epoch: 18,  score: 8.4, loss: 0.047, epsilon: 0.094\n",
      "epoch: 19,  score: 9.3, loss: 0.052, epsilon: 0.094\n",
      "epoch: 20,  score: 9.6, loss: 0.071, epsilon: 0.093\n",
      "epoch: 21,  score: 8.9, loss: 0.060, epsilon: 0.093\n",
      "epoch: 22,  score: 6.8, loss: 0.044, epsilon: 0.093\n",
      "epoch: 23,  score: 11.2, loss: 0.051, epsilon: 0.092\n",
      "epoch: 24,  score: 12.6, loss: 0.080, epsilon: 0.092\n",
      "epoch: 25,  score: 11.6, loss: 0.051, epsilon: 0.092\n",
      "epoch: 26,  score: 13.1, loss: 0.072, epsilon: 0.091\n",
      "epoch: 27,  score: 12.1, loss: 0.056, epsilon: 0.091\n",
      "epoch: 28,  score: 11.9, loss: 0.063, epsilon: 0.091\n",
      "epoch: 29,  score: 11.2, loss: 0.046, epsilon: 0.090\n",
      "epoch: 30,  score: 10.4, loss: 0.066, epsilon: 0.090\n",
      "epoch: 31,  score: 9.5, loss: 0.048, epsilon: 0.090\n",
      "epoch: 32,  score: 13.9, loss: 0.065, epsilon: 0.089\n",
      "epoch: 33,  score: 12.6, loss: 0.052, epsilon: 0.089\n",
      "epoch: 34,  score: 11.7, loss: 0.059, epsilon: 0.089\n",
      "epoch: 35,  score: 9.8, loss: 0.052, epsilon: 0.088\n",
      "epoch: 36,  score: 7.4, loss: 0.041, epsilon: 0.088\n",
      "epoch: 37,  score: 11.4, loss: 0.043, epsilon: 0.088\n",
      "epoch: 38,  score: 12.6, loss: 0.079, epsilon: 0.087\n",
      "epoch: 39,  score: 12.7, loss: 0.074, epsilon: 0.087\n",
      "epoch: 40,  score: 10.4, loss: 0.045, epsilon: 0.087\n",
      "epoch: 41,  score: 10.4, loss: 0.065, epsilon: 0.086\n",
      "epoch: 42,  score: 8.3, loss: 0.056, epsilon: 0.086\n",
      "epoch: 43,  score: 9.5, loss: 0.050, epsilon: 0.086\n",
      "epoch: 44,  score: 11.3, loss: 0.053, epsilon: 0.085\n",
      "epoch: 45,  score: 11.7, loss: 0.067, epsilon: 0.085\n",
      "epoch: 46,  score: 9.8, loss: 0.057, epsilon: 0.085\n",
      "epoch: 47,  score: 7.8, loss: 0.055, epsilon: 0.084\n",
      "epoch: 48,  score: 11.6, loss: 0.053, epsilon: 0.084\n",
      "epoch: 49,  score: 10.4, loss: 0.070, epsilon: 0.084\n",
      "epoch: 50,  score: 9.8, loss: 0.065, epsilon: 0.083\n",
      "epoch: 51,  score: 12.5, loss: 0.066, epsilon: 0.083\n",
      "epoch: 52,  score: 13.5, loss: 0.067, epsilon: 0.083\n",
      "epoch: 53,  score: 11.4, loss: 0.062, epsilon: 0.082\n",
      "epoch: 54,  score: 13.6, loss: 0.081, epsilon: 0.082\n",
      "epoch: 55,  score: 12.2, loss: 0.094, epsilon: 0.082\n",
      "epoch: 56,  score: 11.9, loss: 0.078, epsilon: 0.081\n",
      "epoch: 57,  score: 11.2, loss: 0.044, epsilon: 0.081\n",
      "epoch: 58,  score: 10.3, loss: 0.058, epsilon: 0.081\n",
      "epoch: 59,  score: 11.3, loss: 0.058, epsilon: 0.080\n",
      "epoch: 60,  score: 10.2, loss: 0.061, epsilon: 0.080\n",
      "epoch: 61,  score: 10.7, loss: 0.048, epsilon: 0.080\n",
      "epoch: 62,  score: 9.1, loss: 0.039, epsilon: 0.079\n",
      "epoch: 63,  score: 9.4, loss: 0.047, epsilon: 0.079\n",
      "epoch: 64,  score: 10.5, loss: 0.050, epsilon: 0.079\n",
      "epoch: 65,  score: 11.2, loss: 0.060, epsilon: 0.078\n",
      "epoch: 66,  score: 13.0, loss: 0.065, epsilon: 0.078\n",
      "epoch: 67,  score: 8.7, loss: 0.067, epsilon: 0.078\n",
      "epoch: 68,  score: 10.6, loss: 0.050, epsilon: 0.077\n",
      "epoch: 69,  score: 10.5, loss: 0.058, epsilon: 0.077\n",
      "epoch: 70,  score: 10.1, loss: 0.065, epsilon: 0.077\n",
      "epoch: 71,  score: 10.7, loss: 0.068, epsilon: 0.076\n",
      "epoch: 72,  score: 8.6, loss: 0.067, epsilon: 0.076\n",
      "epoch: 73,  score: 11.8, loss: 0.047, epsilon: 0.076\n",
      "epoch: 74,  score: 10.1, loss: 0.063, epsilon: 0.075\n",
      "epoch: 75,  score: 9.9, loss: 0.053, epsilon: 0.075\n",
      "epoch: 76,  score: 10.9, loss: 0.058, epsilon: 0.075\n",
      "epoch: 77,  score: 11.5, loss: 0.061, epsilon: 0.074\n",
      "epoch: 78,  score: 11.0, loss: 0.058, epsilon: 0.074\n",
      "epoch: 79,  score: 10.9, loss: 0.068, epsilon: 0.074\n",
      "epoch: 80,  score: 12.4, loss: 0.073, epsilon: 0.073\n",
      "epoch: 81,  score: 11.7, loss: 0.077, epsilon: 0.073\n",
      "epoch: 82,  score: 13.7, loss: 0.064, epsilon: 0.073\n",
      "epoch: 83,  score: 13.4, loss: 0.076, epsilon: 0.072\n",
      "epoch: 84,  score: 12.3, loss: 0.047, epsilon: 0.072\n",
      "epoch: 85,  score: 13.7, loss: 0.064, epsilon: 0.072\n",
      "epoch: 86,  score: 13.8, loss: 0.072, epsilon: 0.071\n",
      "epoch: 87,  score: 10.8, loss: 0.075, epsilon: 0.071\n",
      "epoch: 88,  score: 11.8, loss: 0.049, epsilon: 0.071\n",
      "epoch: 89,  score: 12.1, loss: 0.087, epsilon: 0.070\n",
      "epoch: 90,  score: 9.8, loss: 0.072, epsilon: 0.070\n",
      "epoch: 91,  score: 7.3, loss: 0.062, epsilon: 0.070\n",
      "epoch: 92,  score: 10.9, loss: 0.052, epsilon: 0.069\n",
      "epoch: 93,  score: 14.6, loss: 0.058, epsilon: 0.069\n",
      "epoch: 94,  score: 13.2, loss: 0.072, epsilon: 0.069\n",
      "epoch: 95,  score: 14.0, loss: 0.065, epsilon: 0.068\n",
      "epoch: 96,  score: 13.2, loss: 0.055, epsilon: 0.068\n",
      "epoch: 97,  score: 12.9, loss: 0.076, epsilon: 0.068\n",
      "epoch: 98,  score: 13.7, loss: 0.066, epsilon: 0.067\n",
      "epoch: 99,  score: 10.6, loss: 0.083, epsilon: 0.067\n",
      "epoch: 100,  score: 6.6, loss: 0.074, epsilon: 0.067\n",
      "epoch: 101,  score: 8.8, loss: 0.040, epsilon: 0.066\n",
      "epoch: 102,  score: 12.7, loss: 0.055, epsilon: 0.066\n",
      "epoch: 103,  score: 9.8, loss: 0.055, epsilon: 0.066\n",
      "epoch: 104,  score: 11.6, loss: 0.056, epsilon: 0.065\n",
      "epoch: 105,  score: 11.2, loss: 0.062, epsilon: 0.065\n",
      "epoch: 106,  score: 12.0, loss: 0.064, epsilon: 0.065\n",
      "epoch: 107,  score: 10.5, loss: 0.054, epsilon: 0.064\n",
      "epoch: 108,  score: 11.3, loss: 0.067, epsilon: 0.064\n",
      "epoch: 109,  score: 11.0, loss: 0.065, epsilon: 0.064\n",
      "epoch: 110,  score: 13.3, loss: 0.071, epsilon: 0.063\n",
      "epoch: 111,  score: 12.5, loss: 0.054, epsilon: 0.063\n",
      "epoch: 112,  score: 11.9, loss: 0.078, epsilon: 0.063\n",
      "epoch: 113,  score: 14.4, loss: 0.096, epsilon: 0.062\n",
      "epoch: 114,  score: 13.1, loss: 0.081, epsilon: 0.062\n",
      "epoch: 115,  score: 11.6, loss: 0.061, epsilon: 0.062\n",
      "epoch: 116,  score: 9.1, loss: 0.050, epsilon: 0.061\n",
      "epoch: 117,  score: 14.5, loss: 0.079, epsilon: 0.061\n",
      "epoch: 118,  score: 13.5, loss: 0.090, epsilon: 0.061\n",
      "epoch: 119,  score: 13.2, loss: 0.088, epsilon: 0.060\n",
      "epoch: 120,  score: 13.6, loss: 0.093, epsilon: 0.060\n",
      "epoch: 121,  score: 8.5, loss: 0.055, epsilon: 0.060\n",
      "epoch: 122,  score: 10.8, loss: 0.050, epsilon: 0.059\n",
      "epoch: 123,  score: 11.4, loss: 0.046, epsilon: 0.059\n",
      "epoch: 124,  score: 10.6, loss: 0.061, epsilon: 0.059\n",
      "epoch: 125,  score: 15.4, loss: 0.074, epsilon: 0.058\n",
      "epoch: 126,  score: 13.8, loss: 0.100, epsilon: 0.058\n",
      "epoch: 127,  score: 12.5, loss: 0.066, epsilon: 0.058\n",
      "epoch: 128,  score: 11.9, loss: 0.079, epsilon: 0.057\n",
      "epoch: 129,  score: 9.9, loss: 0.080, epsilon: 0.057\n",
      "epoch: 130,  score: 14.4, loss: 0.069, epsilon: 0.057\n",
      "epoch: 131,  score: 11.2, loss: 0.077, epsilon: 0.056\n",
      "epoch: 132,  score: 9.9, loss: 0.066, epsilon: 0.056\n",
      "epoch: 133,  score: 13.3, loss: 0.074, epsilon: 0.056\n",
      "epoch: 134,  score: 10.2, loss: 0.092, epsilon: 0.055\n",
      "epoch: 135,  score: 11.3, loss: 0.088, epsilon: 0.055\n",
      "epoch: 136,  score: 11.1, loss: 0.080, epsilon: 0.055\n",
      "epoch: 137,  score: 11.0, loss: 0.076, epsilon: 0.054\n",
      "epoch: 138,  score: 7.8, loss: 0.052, epsilon: 0.054\n",
      "epoch: 139,  score: 12.3, loss: 0.054, epsilon: 0.054\n",
      "epoch: 140,  score: 13.8, loss: 0.061, epsilon: 0.053\n",
      "epoch: 141,  score: 13.4, loss: 0.063, epsilon: 0.053\n",
      "epoch: 142,  score: 14.8, loss: 0.078, epsilon: 0.053\n",
      "epoch: 143,  score: 11.6, loss: 0.070, epsilon: 0.052\n",
      "epoch: 144,  score: 11.6, loss: 0.064, epsilon: 0.052\n",
      "epoch: 145,  score: 8.1, loss: 0.068, epsilon: 0.052\n",
      "epoch: 146,  score: 10.8, loss: 0.065, epsilon: 0.051\n",
      "epoch: 147,  score: 11.0, loss: 0.051, epsilon: 0.051\n",
      "epoch: 148,  score: 13.6, loss: 0.060, epsilon: 0.051\n",
      "epoch: 149,  score: 14.8, loss: 0.079, epsilon: 0.050\n",
      "epoch: 150,  score: 14.2, loss: 0.066, epsilon: 0.050\n",
      "epoch: 151,  score: 15.6, loss: 0.099, epsilon: 0.050\n",
      "epoch: 152,  score: 15.2, loss: 0.074, epsilon: 0.050\n",
      "epoch: 153,  score: 12.4, loss: 0.103, epsilon: 0.050\n",
      "epoch: 154,  score: 14.6, loss: 0.085, epsilon: 0.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 155,  score: 14.2, loss: 0.068, epsilon: 0.050\n",
      "epoch: 156,  score: 14.2, loss: 0.074, epsilon: 0.050\n",
      "epoch: 157,  score: 15.8, loss: 0.089, epsilon: 0.050\n",
      "epoch: 158,  score: 14.2, loss: 0.097, epsilon: 0.050\n",
      "epoch: 159,  score: 13.7, loss: 0.076, epsilon: 0.050\n",
      "epoch: 160,  score: 14.1, loss: 0.061, epsilon: 0.050\n",
      "epoch: 161,  score: 12.4, loss: 0.060, epsilon: 0.050\n",
      "epoch: 162,  score: 14.8, loss: 0.083, epsilon: 0.050\n",
      "epoch: 163,  score: 15.1, loss: 0.079, epsilon: 0.050\n",
      "epoch: 164,  score: 11.4, loss: 0.061, epsilon: 0.050\n",
      "epoch: 165,  score: 15.3, loss: 0.060, epsilon: 0.050\n",
      "epoch: 166,  score: 14.6, loss: 0.101, epsilon: 0.050\n",
      "epoch: 167,  score: 13.1, loss: 0.087, epsilon: 0.050\n",
      "epoch: 168,  score: 17.9, loss: 0.095, epsilon: 0.050\n",
      "epoch: 169,  score: 13.9, loss: 0.092, epsilon: 0.050\n",
      "epoch: 170,  score: 14.5, loss: 0.080, epsilon: 0.050\n",
      "epoch: 171,  score: 13.2, loss: 0.075, epsilon: 0.050\n",
      "epoch: 172,  score: 14.7, loss: 0.098, epsilon: 0.050\n",
      "epoch: 173,  score: 11.0, loss: 0.066, epsilon: 0.050\n",
      "epoch: 174,  score: 13.3, loss: 0.067, epsilon: 0.050\n",
      "epoch: 175,  score: 16.3, loss: 0.077, epsilon: 0.050\n",
      "epoch: 176,  score: 16.0, loss: 0.091, epsilon: 0.050\n",
      "epoch: 177,  score: 13.1, loss: 0.084, epsilon: 0.050\n",
      "epoch: 178,  score: 15.5, loss: 0.090, epsilon: 0.050\n",
      "epoch: 179,  score: 15.3, loss: 0.090, epsilon: 0.050\n",
      "epoch: 180,  score: 10.2, loss: 0.062, epsilon: 0.050\n",
      "epoch: 181,  score: 16.3, loss: 0.088, epsilon: 0.050\n",
      "epoch: 182,  score: 13.4, loss: 0.109, epsilon: 0.050\n",
      "epoch: 183,  score: 12.9, loss: 0.092, epsilon: 0.050\n",
      "epoch: 184,  score: 14.3, loss: 0.095, epsilon: 0.050\n",
      "epoch: 185,  score: 14.5, loss: 0.075, epsilon: 0.050\n",
      "epoch: 186,  score: 13.2, loss: 0.081, epsilon: 0.050\n",
      "epoch: 187,  score: 13.7, loss: 0.061, epsilon: 0.050\n",
      "epoch: 188,  score: 12.6, loss: 0.073, epsilon: 0.050\n",
      "epoch: 189,  score: 13.8, loss: 0.088, epsilon: 0.050\n",
      "epoch: 190,  score: 14.6, loss: 0.077, epsilon: 0.050\n",
      "epoch: 191,  score: 11.4, loss: 0.073, epsilon: 0.050\n",
      "epoch: 192,  score: 12.9, loss: 0.085, epsilon: 0.050\n",
      "epoch: 193,  score: 12.6, loss: 0.084, epsilon: 0.050\n",
      "epoch: 194,  score: 11.4, loss: 0.074, epsilon: 0.050\n",
      "epoch: 195,  score: 14.1, loss: 0.093, epsilon: 0.050\n",
      "epoch: 196,  score: 9.5, loss: 0.105, epsilon: 0.050\n",
      "epoch: 197,  score: 10.5, loss: 0.070, epsilon: 0.050\n",
      "epoch: 198,  score: 13.4, loss: 0.065, epsilon: 0.050\n",
      "epoch: 199,  score: 14.9, loss: 0.069, epsilon: 0.050\n"
     ]
    }
   ],
   "source": [
    "epsilon_max = 0.9            #value of epsilon when training is started\n",
    "epsilon_min = 0.05           #value of epsilon when training has reached epsilon_decay_epochs\n",
    "epsilon_decay_epochs = 200   #number if epochs requred to reach epsilon_min\n",
    "TAU = .005                   #controls degree to which target models are updated after each game.\n",
    "gamma = .99                  #discounted rewards factor\n",
    "\n",
    "epochs = 250                 #how many epochs to run for (200 seems to be about enough)\n",
    "games_per_epoch = 10         #games played per epoch\n",
    "batch_trainings_per_game = 4 #how many training batches are performed after each game (games last about 300 steps, so 4*64 sounds about right)\n",
    "batch_size = 64              #training batch size\n",
    "\n",
    "learning_rate = .0005        #learning rate.  .0005 seems to work OK.  \n",
    "optimizer = torch.optim.Adam(banana_model_local.parameters(), lr=learning_rate)  \n",
    "\n",
    "for epoch in range(epochs):\n",
    "    #calculate epsilon for this epoch.\n",
    "    epsilon = epsilon_max - (epsilon_max-epsilon_min)*(epoch/epsilon_decay_epochs)\n",
    "    epsilon = max(epsilon_min, epsilon)\n",
    "    total_loss = 0\n",
    "    total_score = 0\n",
    "    \n",
    "    for game_count in range(games_per_epoch):\n",
    "        #play a game, adding BananaStepInfo steps to step_history\n",
    "        score = play_game(env, banana_model_local, epsilon, brain_name, step_history)\n",
    "        total_score += score\n",
    "        \n",
    "        for batch_training_count in range(batch_trainings_per_game):\n",
    "            optimizer.zero_grad() #zero out gradients.\n",
    "            \n",
    "            #get training batch\n",
    "            state_batch, next_state_batch, actions_batch, reward_batch, done_batch = get_training_batch(batch_size, step_history)\n",
    "\n",
    "            #now, implement Double DQN strategy using one target model to get maximum actions and another target \n",
    "            #model to get the expected rewards based upon those actions.  Using the two models in this manner helps \n",
    "            #to ensure rewards that deviate far from reality do not negatively influence training.\n",
    "            \n",
    "            #using target model 1, get the greedy actions (actions with greatest reward) for each step in the batch (target_indices_1).\n",
    "            next_state_tensor = torch.from_numpy(next_state_batch).float().cuda()\n",
    "            target_q_values_next_1 = banana_model_target_1(next_state_tensor)\n",
    "            target_q_values_next_max_1, target_indices_1 = torch.max(target_q_values_next_1, dim=1, keepdim=True)\n",
    "            \n",
    "            #using the greedy actions (target_indices_1) from target model 1, get the expected rewards from target model 2 (target_q_values_next)\n",
    "            target_q_values_next_2 = banana_model_target_2(next_state_tensor)\n",
    "            target_q_values_next = target_q_values_next_2.gather(1, target_indices_1)\n",
    "            \n",
    "            #get the expected current rewards earned using historical actions performed using the banana_model_local\n",
    "            history_actions_batch_reshaped = np.reshape(actions_batch, (batch_size, 1))\n",
    "            history_actions_batch_tensor = torch.from_numpy(history_actions_batch_reshaped).long().cuda()\n",
    "            state_tensor = torch.from_numpy(state_batch).float().cuda()\n",
    "            local_q_values = banana_model_local(state_tensor)  #feed historical states into local model to get q_values for each action \n",
    "            local_q_values_performed = local_q_values.gather(1, history_actions_batch_tensor) #get q_value for each action that was performed from history\n",
    "\n",
    "            #get the actual rewards earned from the playthrough history\n",
    "            reward_batch_reshaped = np.reshape(reward_batch, (batch_size, 1))\n",
    "            reward_batch_tensor = torch.from_numpy(reward_batch_reshaped).float().cuda()\n",
    "\n",
    "            #calculate loss using the deep Q learning equation.  I like seeing the actual equation this way instead of breaking\n",
    "            #it up across multiple lines.\n",
    "            loss = torch.mean((reward_batch_tensor + (gamma * target_q_values_next) - local_q_values_performed)**2)\n",
    "            total_loss += loss.item() #track total loss\n",
    "\n",
    "            loss.backward()   #backpropigation is only performed on the local model\n",
    "            optimizer.step()\n",
    "    \n",
    "        #now, randomly choose a taret model to update. Randomly choosing a model makes sure one model is not influenced \n",
    "        #too much to negate the traing stability that Doubel DQN wants to achieve.\n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand < .5:\n",
    "            soft_update_target(banana_model_local, banana_model_target_1, TAU)\n",
    "        else:\n",
    "            soft_update_target(banana_model_local, banana_model_target_2, TAU)\n",
    "        \n",
    "    print(\"epoch: {},  score: {}, loss: {:.3f}, epsilon: {:.3f}\".format(epoch, total_score/games_per_epoch, total_loss/games_per_epoch, epsilon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you like the model, save it here\n",
    "torch.save(banana_model_local.state_dict(), \"banana_model.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BananaModel(\n",
       "  (fc1): Linear(in_features=37, out_features=128, bias=True)\n",
       "  (fc2): Linear(in_features=128, out_features=128, bias=True)\n",
       "  (out): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the weights into a new model to play a game and get the score\n",
    "banana_model_load  = BananaModel(state_size, action_size).cuda()\n",
    "banana_model_load.load_state_dict(torch.load(\"banana_model.pt\"));\n",
    "\n",
    "print(banana_model_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#play a game to see what score is achieved\n",
    "\n",
    "play_history = []\n",
    "score = play_game(env, banana_model_load, .01, brain_name, play_history)\n",
    "\n",
    "print(\"Score {} in {} steps\".format(score, len(play_history)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
