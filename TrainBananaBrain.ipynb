{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.4 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain Name: BananaBrain\n",
      "Number of agents: 1\n",
      "Number of actions (action_size): 4\n",
      "State size (state_size): 37\n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "\n",
    "#collect infomration about the envronment\n",
    "# reset the environment\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]\n",
    "print('Brain Name:', brain_name)\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions (action_size):', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "#print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('State size (state_size):', state_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#define the NN Model\n",
    "class BananaModel(torch.nn.Module):\n",
    "    def __init__(self, state_size, action_count, dropout=0.25):\n",
    "        super(BananaModel, self).__init__()\n",
    "        self.state_size   = state_size\n",
    "        self.action_count = action_count\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        self.out = torch.nn.Linear(128, action_count)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    \n",
    "def soft_update_target(local_model, target_model, tau):\n",
    "    \"\"\"Soft update model parameters.\n",
    "    θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "    Params\n",
    "    ======\n",
    "        local_model (PyTorch model): weights will be copied from\n",
    "        target_model (PyTorch model): weights will be copied to\n",
    "        tau (float): interpolation parameter \n",
    "    \"\"\"\n",
    "    for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "        target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class BananaStepInfo():\n",
    "        def __init__(self, action, reward, state, next_state, done):\n",
    "            self.action = action\n",
    "            self.reward = reward\n",
    "            self.state = state\n",
    "            self.next_state = next_state\n",
    "            self.done = done\n",
    "            \n",
    "        def to_string(self, show_states=False):\n",
    "            if show_states:\n",
    "                val = \"BananaStepInfo[action: {}, reward: {}, done: {}, state: {}, next_state: {}]\".format(self.action, self.reward, self.done, self.state, self.next_state)\n",
    "            else:\n",
    "                val = \"BananaStepInfo[action: {}, reward: {}, done: {}]\".format(self.action, self.reward, self.done)\n",
    "            return val\n",
    "\n",
    "        \n",
    "def play_game(env, banana_model, epsilon, brain_name, history):\n",
    "    score = 0\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    state = env_info.vector_observations[0]            # get the current state\n",
    "    while True:\n",
    "        \n",
    "        rand = random.uniform(0, 1)\n",
    "        if rand < epsilon:\n",
    "            action = np.random.randint(action_size)\n",
    "        else:\n",
    "            state_tensor = torch.from_numpy(state).float().cuda()\n",
    "            state_tensor = state_tensor.view((1,)+state.shape) #reshape for batch size of 1\n",
    "            output = model(state_tensor)\n",
    "            action_array = output.detach().cpu().numpy()[0]\n",
    "            action = np.argmax(action_array)\n",
    "            \n",
    "        # select an action TODO - use the model\n",
    "        env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "        next_state = env_info.vector_observations[0]   # get the next state\n",
    "        reward = env_info.rewards[0]                   # get the reward\n",
    "        score += reward\n",
    "        done = env_info.local_done[0]                  # see if episode has finished\n",
    "        \n",
    "        bananaStepInfo = BananaStepInfo(action, reward, state, next_state, done)\n",
    "        history.append(bananaStepInfo)\n",
    "        state = next_state\n",
    "\n",
    "        if done:                                       # exit loop if episode finished\n",
    "            break\n",
    "    return score\n",
    "            \n",
    "def get_training_batch(batch_size, history):\n",
    "    rand_arr = np.arange(len(history))\n",
    "    np.random.shuffle(rand_arr)\n",
    "   \n",
    "    index_counter = 0\n",
    "    batch_index_counter = 0\n",
    "    \n",
    "    state_batch        = np.zeros((batch_size, state_size))\n",
    "    next_state_batch   = np.zeros((batch_size, state_size))\n",
    "    reward_batch       = np.zeros(batch_size)\n",
    "    actions_batch      = np.zeros(batch_size)\n",
    "    done_batch         = np.zeros(batch_size)\n",
    "    \n",
    "    for batch_index in range(batch_size):\n",
    "        frame_number = rand_arr[batch_index]\n",
    "        bananaStepInfo = history[frame_number]\n",
    "        \n",
    "        state_batch[batch_index]      = bananaStepInfo.state\n",
    "        next_state_batch[batch_index] = bananaStepInfo.next_state\n",
    "        reward_batch[batch_index]     = bananaStepInfo.reward\n",
    "        actions_batch[batch_index]    = bananaStepInfo.action\n",
    "        done_batch[batch_index]       = bananaStepInfo.done\n",
    "\n",
    "    return state_batch, next_state_batch, actions_batch, reward_batch, done_batch\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: \n",
      "0: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "1: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "2: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "3: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "4: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "5: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "6: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "7: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "8: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "9: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "10: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "11: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "12: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "13: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "14: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "15: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "16: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "17: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "18: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "19: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "20: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "21: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "22: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "23: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "24: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "25: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "26: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "27: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "28: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "29: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "30: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "31: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "32: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "33: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "34: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "35: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "36: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "37: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "38: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "39: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "40: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "41: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "42: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "43: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "44: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "45: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "46: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "47: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "48: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "49: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "50: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "51: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "52: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "53: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "54: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "55: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "56: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "57: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "58: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "59: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "60: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "61: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "62: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "63: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "64: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "65: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "66: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "67: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "68: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "69: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "70: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "71: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "72: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "73: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "74: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "75: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "76: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "77: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "78: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "79: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "80: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "81: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "82: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "83: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "84: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "85: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "86: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "87: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "88: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "89: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "90: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "91: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "92: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "93: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "94: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "95: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "96: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "97: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "98: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "99: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "100: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "101: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "102: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "103: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "104: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "105: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "106: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "107: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "108: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "109: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "110: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "111: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "112: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "113: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "114: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "115: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "116: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "117: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "118: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "119: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "120: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "121: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "122: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "123: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "124: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "125: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "126: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "127: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "128: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "129: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "130: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "131: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "132: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "133: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "134: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "135: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "136: BananaStepInfo[action: 0, reward: 1.0, done: False]\n",
      "137: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "138: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "139: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "140: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "141: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "142: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "143: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "144: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "145: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "146: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "147: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "148: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "149: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "150: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "151: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "152: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "153: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "154: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "155: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "156: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "157: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "158: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "159: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "160: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "161: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "162: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "163: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "164: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "165: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "166: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "167: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "168: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "169: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "170: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "171: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "172: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "173: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "174: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "175: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "176: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "177: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "178: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "179: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "180: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "181: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "182: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "183: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "184: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "185: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "186: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "187: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "188: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "189: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "190: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "191: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "192: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "193: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "194: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "195: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "196: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "197: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "198: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "199: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "200: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "201: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "202: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "203: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "204: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "205: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "206: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "207: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "208: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "209: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "210: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "211: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "212: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "213: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "214: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "215: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "216: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "217: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "218: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "219: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "220: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "221: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "222: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "223: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "224: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "225: BananaStepInfo[action: 0, reward: -1.0, done: False]\n",
      "226: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "227: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "228: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "229: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "230: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "231: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "232: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "233: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "234: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "235: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "236: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "237: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "238: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "239: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "240: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "241: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "242: BananaStepInfo[action: 0, reward: -1.0, done: False]\n",
      "243: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "244: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "245: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "246: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "247: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "248: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "249: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "250: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "251: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "252: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "253: BananaStepInfo[action: 3, reward: 1.0, done: False]\n",
      "254: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "255: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "256: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "257: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "258: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "259: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "260: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "261: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "262: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "263: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "264: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "265: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "266: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "267: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "268: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "269: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "270: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "271: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "272: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "273: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "274: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "275: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "276: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "277: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "278: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "279: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "280: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "281: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "282: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "283: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "284: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "285: BananaStepInfo[action: 1, reward: 0.0, done: False]\n",
      "286: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "287: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "288: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "289: BananaStepInfo[action: 3, reward: 0.0, done: False]\n",
      "290: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "291: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "292: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "293: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "294: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "295: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "296: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "297: BananaStepInfo[action: 0, reward: 0.0, done: False]\n",
      "298: BananaStepInfo[action: 2, reward: 0.0, done: False]\n",
      "299: BananaStepInfo[action: 3, reward: 0.0, done: True]\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "0.0\n",
      "2.0\n",
      "0.0\n",
      "0.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "2.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "3.0\n",
      "2.0\n",
      "3.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "3.0\n",
      "3.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "2.0\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "#test out the functions\n",
    "step_history = deque(maxlen=1000)\n",
    "       \n",
    "model = BananaModel(state_size, action_size).cuda()\n",
    "score = play_game(env, model, .5, brain_name, step_history)\n",
    "print(\"score: \".format(score))\n",
    "\n",
    "state_batch, next_state_batch, actions_batch, reward_batch, done_batch = get_training_batch(64, step_history)\n",
    "\n",
    "for i, banana_step in enumerate(step_history):\n",
    "    print(\"{}: {}\".format(i, banana_step.to_string(show_states=False)))\n",
    "    \n",
    "for state, next_state, action, reward, done in zip(state_batch, next_state_batch, actions_batch, reward_batch, done_batch):\n",
    "  print(action) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create model,optimizer\n",
    "banana_model_local  = BananaModel(state_size, action_size).cuda()\n",
    "banana_model_target = BananaModel(state_size, action_size).cuda()\n",
    "step_history = deque(maxlen=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0,  score: 0.0, loss: 0.026, epsilon: 0.800\n",
      "epoch: 1,  score: 0.0, loss: 0.023, epsilon: 0.777\n",
      "epoch: 2,  score: 0.4, loss: 0.016, epsilon: 0.753\n",
      "epoch: 3,  score: 0.2, loss: 0.036, epsilon: 0.730\n",
      "epoch: 4,  score: 0.3, loss: 0.013, epsilon: 0.707\n",
      "epoch: 5,  score: 0.7, loss: 0.011, epsilon: 0.683\n",
      "epoch: 6,  score: 0.0, loss: 0.035, epsilon: 0.660\n",
      "epoch: 7,  score: 0.4, loss: 0.032, epsilon: 0.637\n",
      "epoch: 8,  score: 0.7, loss: 0.030, epsilon: 0.613\n",
      "epoch: 9,  score: 0.6, loss: 0.033, epsilon: 0.590\n",
      "epoch: 10,  score: 0.7, loss: 0.014, epsilon: 0.567\n",
      "epoch: 11,  score: 0.2, loss: 0.023, epsilon: 0.543\n",
      "epoch: 12,  score: 1.1, loss: 0.026, epsilon: 0.520\n",
      "epoch: 13,  score: 0.8, loss: 0.024, epsilon: 0.497\n",
      "epoch: 14,  score: 0.1, loss: 0.020, epsilon: 0.473\n",
      "epoch: 15,  score: 0.2, loss: 0.026, epsilon: 0.450\n",
      "epoch: 16,  score: 1.1, loss: 0.040, epsilon: 0.427\n",
      "epoch: 17,  score: 1.2, loss: 0.037, epsilon: 0.403\n",
      "epoch: 18,  score: 0.6, loss: 0.057, epsilon: 0.380\n",
      "epoch: 19,  score: 2.2, loss: 0.029, epsilon: 0.357\n",
      "epoch: 20,  score: 1.4, loss: 0.029, epsilon: 0.333\n",
      "epoch: 21,  score: 0.5, loss: 0.027, epsilon: 0.310\n",
      "epoch: 22,  score: 0.1, loss: 0.032, epsilon: 0.287\n",
      "epoch: 23,  score: 0.0, loss: 0.022, epsilon: 0.263\n",
      "epoch: 24,  score: 0.3, loss: 0.038, epsilon: 0.240\n",
      "epoch: 25,  score: 0.1, loss: 0.029, epsilon: 0.217\n",
      "epoch: 26,  score: 0.8, loss: 0.024, epsilon: 0.193\n",
      "epoch: 27,  score: -0.2, loss: 0.030, epsilon: 0.170\n",
      "epoch: 28,  score: -0.2, loss: 0.022, epsilon: 0.147\n",
      "epoch: 29,  score: 0.3, loss: 0.020, epsilon: 0.123\n",
      "epoch: 30,  score: 0.4, loss: 0.020, epsilon: 0.100\n",
      "epoch: 31,  score: 0.4, loss: 0.022, epsilon: 0.100\n",
      "epoch: 32,  score: 1.7, loss: 0.035, epsilon: 0.100\n",
      "epoch: 33,  score: 0.3, loss: 0.029, epsilon: 0.100\n",
      "epoch: 34,  score: 0.4, loss: 0.024, epsilon: 0.100\n",
      "epoch: 35,  score: 0.9, loss: 0.022, epsilon: 0.100\n",
      "epoch: 36,  score: 0.5, loss: 0.024, epsilon: 0.100\n",
      "epoch: 37,  score: 0.8, loss: 0.049, epsilon: 0.100\n",
      "epoch: 38,  score: -0.5, loss: 0.030, epsilon: 0.100\n",
      "epoch: 39,  score: 0.0, loss: 0.024, epsilon: 0.100\n",
      "epoch: 40,  score: 1.0, loss: 0.019, epsilon: 0.100\n",
      "epoch: 41,  score: 0.3, loss: 0.030, epsilon: 0.100\n",
      "epoch: 42,  score: 1.1, loss: 0.043, epsilon: 0.100\n",
      "epoch: 43,  score: 0.4, loss: 0.032, epsilon: 0.100\n",
      "epoch: 44,  score: 0.4, loss: 0.025, epsilon: 0.100\n",
      "epoch: 45,  score: 0.9, loss: 0.024, epsilon: 0.100\n",
      "epoch: 46,  score: 0.4, loss: 0.016, epsilon: 0.100\n",
      "epoch: 47,  score: 0.6, loss: 0.033, epsilon: 0.100\n",
      "epoch: 48,  score: 0.5, loss: 0.014, epsilon: 0.100\n",
      "epoch: 49,  score: 0.6, loss: 0.030, epsilon: 0.100\n"
     ]
    }
   ],
   "source": [
    "epsilon_min = 0.1\n",
    "epsilon_max = 0.8\n",
    "epsilon_decay_epochs = 30\n",
    "epochs = 50\n",
    "games_per_epoch = 10\n",
    "batch_trainings_per_game = 4\n",
    "batch_size = 64\n",
    "learning_rate = .005\n",
    "optimizer = torch.optim.Adam(banana_model_local.parameters(), lr=learning_rate)\n",
    "TAU = .001\n",
    "gamma = .99\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    epsilon = epsilon_max - (epsilon_max-epsilon_min)*(epoch/epsilon_decay_epochs)\n",
    "    epsilon = max(epsilon_min, epsilon)\n",
    "    total_loss = 0\n",
    "    total_score = 0\n",
    "    \n",
    "    for game_count in range(games_per_epoch):\n",
    "        optimizer.zero_grad()\n",
    "        score = play_game(env, banana_model_local, epsilon, brain_name, step_history)\n",
    "        total_score += score\n",
    "        \n",
    "        for batch_training_count in range(batch_trainings_per_game):\n",
    "            state_batch, next_state_batch, actions_batch, reward_batch, done_batch = get_training_batch(batch_size, step_history)\n",
    "\n",
    "            #get the expected reward of the next state from the target banana model\n",
    "            next_state_tensor = torch.from_numpy(next_state_batch).float().cuda()\n",
    "            target_q_values_next = banana_model_target(next_state_tensor)\n",
    "            target_q_values_next_max, indices = torch.max(target_q_values_next, dim=1, keepdim=True)\n",
    "            #print(\"target_q_values_next_max[:10]: {}\".format(target_q_values_next_max[:10]))\n",
    "\n",
    "            #get the expected rewards earned using actions performed using the local banana model\n",
    "            actions_batch_reshaped = np.reshape(actions_batch, (batch_size, 1))\n",
    "            actions_batch_tensor = torch.from_numpy(actions_batch_reshaped).long().cuda()\n",
    "            state_tensor = torch.from_numpy(state_batch).float().cuda()\n",
    "            local_q_values = banana_model_local(state_tensor)\n",
    "            local_q_values_performed = local_q_values.gather(1, actions_batch_tensor)\n",
    "            #print(\"local_q_values_performed[:10]: {}\".format(local_q_values_performed[:10]))\n",
    "\n",
    "            #get the actual rewards earned \n",
    "            reward_batch_reshaped = np.reshape(reward_batch, (batch_size, 1))\n",
    "            reward_batch_tensor = torch.from_numpy(reward_batch_reshaped).float().cuda()\n",
    "            #print(\"reward_batch_tensor[:10]: {}\".format(reward_batch_tensor[:10]))\n",
    "\n",
    "            #calculate loss\n",
    "            loss = torch.mean((reward_batch_tensor + (gamma * target_q_values_next_max) - local_q_values_performed)**2)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "        soft_update_target(banana_model_local, banana_model_target, TAU)\n",
    "        \n",
    "    print(\"epoch: {},  score: {}, loss: {:.3f}, epsilon: {:.3f}\".format(epoch, total_score/games_per_epoch, total_loss/games_per_epoch, epsilon))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
